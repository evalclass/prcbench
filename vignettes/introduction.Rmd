---
title: "Introduction to prcbench"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to prcbench}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `prcbench` package provides a testing workbench for evaluating 
Precision-Recall curves.

## 1. Tool interface
The `prcbench` package provides pre-defined interfaces for the following five 
tools that calculate Precision-Recall curves.

Tool          Language  Link                                                     
------------- --------- --------------------------------------------------------
ROCR          R         [CRAN](https://cran.r-project.org/web/packages/ROCR) 
AUCCalculator Java      [Tool web site](http://mark.goadrich.com/programs/AUC)
PerfMeas      R         [CRAN](https://cran.r-project.org/web/packages/PerfMeas)
PRROC         R         [CRAN](https://cran.r-project.org/web/packages/PRROC)
precrec       R         [CRAN](https://cran.r-project.org/web/packages/precrec)  

### Tool set
The `create_toolset` function generates a tool set with any combination of the five tools.

```{r}
library(prcbench)

## A single tool
toolsetA <- create_toolset("ROCR")

## Multiple tools
toolsetB <- create_toolset(c("PerfMeas", "PRROC"))
                           
## Tool sets can be combined
toolsetAB <- c(toolsetA, toolsetB)
```

### Predefined tool sets
The `create_toolset` function takes two additional arguments - `calc_auc` and `store_res`. The `calc_auc` argument makes tools calculate and retrieve the AUC score, whereas the `store_res` argument forces tools to calculate and store the curve values.

The following six tool set names are predefined with different combinations of tools and argument values.

Set name  Tools                                          calc\_auc  store\_res
--------- ---------------------------------------------- ---------- -----------
def5      ROCR, AUCCalculator, PerfMeas, PRROC, precrec  TRUE       TRUE
auc5      ROCR, AUCCalculator, PerfMeas, PRROC, precrec  TRUE       FALSE
crv5      ROCR, AUCCalculator, PerfMeas, PRROC, precrec  FALSE      TRUE
def4      ROCR, AUCCalculator, PerfMeas, precrec         TRUE       TRUE
auc4      ROCR, AUCCalculator, PerfMeas, precrec         TRUE       FALSE
crv4      ROCR, AUCCalculator, PerfMeas, precrec         FALSE      TRUE

```{r}
## Use 'set_names'
toolsetC <- create_toolset(set_names = "auc5")

## Multiple sets are automatically combined
toolsetD <- create_toolset(set_names = c("auc5", "crv4"))
```

## 2. Test data interface
The `prcbench` package provides two different types of test data. The first one
is for benchmarking, and the scound one is for evaluating the accuracy of
Precision-Recall curves.

The `create_testset` function provides both types of test data by setting the first argument either as "bench" or "curve".

### Test datasets for benchmarking
The `create_testset` function uses a name convension for randomly generated data for benchmarking. The format is a prefix ('i' or 'b') followed by the number of dataset. The  prefix 'i' indicates a balanced dataset, whereas 'b' indicates an imbalanced dataset. The number can be used with a suffix 'k' or 'm', indicating respectively 1000 or 1 million.

```{r}
## A balanced data set with 50 positives and 50 negatives
testset1A <- create_testset("bench", "b100")

## An imbalanced data set with 2500 positives and 7500 negatives
testset1B <- create_testset("bench", "i10k")

## Test data sets can be combined
testset1AB <- c(testset1A, testset1B)

## Multiple sets are automatically combined
testset1C <- create_testset("bench", c("i10", "b10"))
```

### Test datasets for curve evaluation
The `create_testset` function takes pre-defined set names for curve evaluation. These data sets contain pre-calculated precision and recall values. The pre-calculated values must be correct so that they can be compared with the results of specified tools.

The following three test sets are currrently available.
- c1 or C1
- c2 or C2
- c3 or C3

```{r}
## C1 test set
testset2A <- create_testset("curve", "c1")

## C2 test set
testset2B <- create_testset("curve", "c2")

## Test data sets can be combined
testset2AB <- c(testset2A, testset2B)

## Multiple sets are automatically combined
testset2C <- create_testset("curve", c("c1", "c2"))
```

## 3. Benchmarking
The `run_benchmark` function intenrally calls the `microbenchmark` 
function provided by the [microbenchmark](https://cran.r-project.org/web/packages/microbenchmark) 
package. It takes a test set and a tool set and returns the result of 
`microbenchmark`.

```{r}
## Run microbenchmark for precrec and b100 test set
testset <- create_testset("bench", "b100")
toolset <- create_toolset("precrec")
run_benchmark(testset, toolset)
```

## 4. Evaluation of the accuracy Precision-Recall curves
The `run_evalcurve` function evaluates Precision-Recall curves with pre-defined
test cases. It evaluates a Precision-Recall curve with five different test cases.

Test case  Description
---------- ---------------------------------------
x_range    Evaluate the range of recall values
y_range    Evaluate the range of precision values
fpoint     Check the first point
int_pts    Check all intermediate points
epoint     Check the end point

### Evaluation scores
The `run_evalcurve` function calculates the scores for test cases and summarizes them to a data frame.

```{r}
## Run microbenchmark for precrec and b100 test set
testset <- create_testset("curve", "c1")
toolset <- create_toolset(c("ROCR", "precrec"))
scores <- run_evalcurve(testset, toolset)
```

### Visualization of the result
The `autoplot` shows a plot with the result of the `run_evalcurve` 
function.
```{r, fig.width=7, warning=FALSE, fig.show='hold'}
## ggplot2 is nesessary to use autoplot
library(ggplot2)

## Plot base points and the reulst of precrec on c1, c2, and c3 test sets
testset <- create_testset("curve", c("c1", "c2", "c3"))
toolset <- create_toolset("precrec")
scores1 <- run_evalcurve(testset, toolset)
autoplot(scores1)

## Plot the results of PerfMeas and PRROC on c1, c2, and c3 test sets
toolset <- create_toolset(c("PerfMeas", "PRROC"))
scores2 <- run_evalcurve(testset, toolset)
autoplot(scores2, base_plot = FALSE)
```

## 5. R6 class
Both tool and test data interfaces are internally implemented as `R6` classes provided by the [R6](https://cran.r-project.org/web/packages/R6) package.

### Tool interface

Tool           R6 class           Parent R6 class                 
-------------- ------------------ -----------------
ROCR           ToolROCR           ToolIFBase
AUCCalculator  ToolAUCCalculator  ToolIFBase
PerfMeas       ToolPerfMeas       ToolIFBase
PRROC          ToolPRROC          ToolIFBase
precrec        Toolprecrec        ToolIFBase

All tool classes inherit the following methods from the `ToolIFBase` class.
    
Method                                Description 
------------------------------------- -----------------------------------
call(testset, calc\_auc, store\_res)  Calculate a Precision-Recall curve 
get_toolname()                        Return the name of the tool
get_setname()                         Return the name of the toolset
get_result()                          Return a list of calculation result
get_x()                               Return a vector of recall values 
get_y()                               Return a vector of precision values 
get_auc()                             Return an AUC score

See the help files of the R6 classes, for instance `help(ToolIFBase)` and 
`help(ToolROCR)`, for more details.  

### Test data interface

Test type         R6 class           Parent R6 class                 
----------------- ------------------ -----------------
benchmarking      TestDataB          -
curve evaluation  TestDataC          TestDataB

## 6. External links
See our website - [Classifier evaluation with imbalanced datasets](https://classeval.wordpress.com/) - for useful tips for performance evaluation on binary classifiers. In addition, we have summarized potential pitfalls of ROC plots with imbalanced datasets. See our paper - [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432) - for more details.
